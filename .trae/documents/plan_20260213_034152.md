# LLM 错误信息优化计划

目前用户在使用官方模型名称 "Qwen3-Max" 时依然报错，但错误信息仅显示 "Network Error" 或 "500 Internal Server Error"，缺乏具体的 API 响应内容（如 404 Model Not Found），导致用户无法自行排查模型名称是否正确。

## 1. 后端：透传详细的 API 错误信息
目前 `LLMService` 使用 `response.raise_for_status()` 直接抛出异常，丢失了 Response Body 中的详细错误信息。

*   **修改文件**: `backend/app/services/llm_service.py`
*   **优化内容**:
    *   在检查 `response.status_code != 200` 时，手动读取 `response.text`。
    *   构造包含状态码和响应体的详细错误消息。
    *   抛出 `ValueError` 或自定义异常，确保这个详细消息能被上层捕获。

## 2. 后端：确保错误信息能返回给前端
目前 `routers/sentiment.py` 已经捕获了异常并返回 200 状态码 + 错误信息，但需要确认 `llm_service` 抛出的详细异常能被正确转换为字符串。

*   **修改文件**: `backend/app/routers/sentiment.py` (无需修改，现有逻辑 `message=str(e)` 已经足够，前提是 `e` 包含详细信息)。

## 3. 验证计划
1.  **错误测试**: 在配置中故意填错模型名称（如 "Qwen3-Max-Fake"），点击生成，验证前端弹窗是否显示 "LLM API Error: 404 - { ... }"。
2.  **正常测试**: 使用正确的 "qwen-max"，验证是否成功。

注意：根据之前的测试，"Qwen3-Max" 确实是错误的 API 模型名称，正确的应该是 "qwen-max"。优化后的报错信息将直接证实这一点。
